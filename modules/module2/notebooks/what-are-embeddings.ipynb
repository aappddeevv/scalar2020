{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Embeddings Using Tensorflow\n",
    "\n",
    "We will create some simple embeddings to help illustrate how to change text to numbers so we can process it.\n",
    "\n",
    "You can watch https://www.youtube.com/watch?v=fNxaJsNG3-s to learn about this more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization - Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "!pip install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Please use a spoon\",\n",
    "    \"Please use a napkin!\",\n",
    "    \"Let's go to the store and buy some groceries\",\n",
    "    \"The park is closed at night.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index list all the words in the sentences and assigns them a number, but that's not enough for processing. You should also notice that these particular functions removed non-character values such as `!` from the text automatically. Not all functions do this so sometimes you need to prep your data more carefully.\n",
    "\n",
    "You may also notice that `Let's` was not changed and the apostrophe was included in the \"word\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'please': 1, 'use': 2, 'a': 3, 'the': 4, 'spoon': 5, 'napkin': 6, \"let's\": 7, 'go': 8, 'to': 9, 'store': 10, 'and': 11, 'buy': 12, 'some': 13, 'groceries': 14, 'park': 15, 'is': 16, 'closed': 17, 'at': 18, 'night': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences (Sequences of Words) and What's `<OOV>`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence is now a sequence of numbers based on the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  [1, 2, 3, 5]\n",
      "1 :  [1, 2, 3, 6]\n",
      "2 :  [7, 8, 9, 4, 10, 11, 12, 13, 14]\n",
      "3 :  [4, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(sequences):\n",
    "    print(i, \": \", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two sequences seem close as they both start with `Please use a`. We could calculate the \"similarity\" of the first sentence to the other sentences and see how they all compare using cosine simularity. However, the sentences have different lengths and other differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4 0 0\n",
      "target:  [1, 2, 3, 5] , comparison:  [1, 2, 3, 5] , sim:  tf.Tensor(-1.0000001, shape=(), dtype=float32)\n",
      "4 4 4 0 0\n",
      "target:  [1, 2, 3, 5] , comparison:  [1, 2, 3, 6] , sim:  tf.Tensor(-0.9964039, shape=(), dtype=float32)\n",
      "4 9 9 5 0\n",
      "target:  [1, 2, 3, 5] , comparison:  [7, 8, 9, 4, 10, 11, 12, 13, 14] , sim:  tf.Tensor(-0.36559632, shape=(), dtype=float32)\n",
      "4 6 6 2 0\n",
      "target:  [1, 2, 3, 5] , comparison:  [4, 15, 16, 17, 18, 19] , sim:  tf.Tensor(-0.6972329, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "target = sequences[0]\n",
    "for i,s in enumerate(sequences):\n",
    "    len0 = len(target)\n",
    "    lens = len(s)\n",
    "    m = max(len0,lens)\n",
    "    pad0 = max(m-len0, 0)\n",
    "    pads = max(m-lens, 0)\n",
    "    print(len0,lens, m, pad0, pads)\n",
    "    x = tf.pad(tf.constant(target, dtype=tf.float32), paddings=tf.constant([[0,pad0]]))\n",
    "    y =  tf.pad(tf.constant(s, dtype=tf.float32), paddings=tf.constant([[0,pads]]))\n",
    "    cos = tf.losses.cosine_similarity(tf.nn.l2_normalize(x,0), tf.nn.l2_normalize(y,0))\n",
    "    print(\"target: \", target, \", comparison: \", s, \", sim: \", cos)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should also notice the funky syntax for tf. They are trying to improve it but it turns out that in order to allow optimizations in the lower layers, you have to tag almost everything. The new swift version makes this a bit easier. The older, \"submit/session\" style is very \"functional\" in its approach in terms of building up a computation as a value then running it later. With the new tf-swift, the \"computation as value\" will be pushed down into a machine learning intermediate representation (MSIR) layer exposing mostly an imperative API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know try to sequence a sentence with words that have not been seen before, the word is essentially dropped in the output vector. So you need to have a large index to capture all the possible world. But a large dictionary can never be large enough. Let's create a new tokenizer that has a placeholder for missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer2.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'please': 2, 'use': 3, 'a': 4, 'the': 5, 'spoon': 6, 'napkin': 7, \"let's\": 8, 'go': 9, 'to': 10, 'store': 11, 'and': 12, 'buy': 13, 'some': 14, 'groceries': 15, 'park': 16, 'is': 17, 'closed': 18, 'at': 19, 'night': 20}\n",
      "[[1, 5, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\"Soon, the moon shall rise\"]\n",
    "test_seq = tokenizer2.texts_to_sequences(test_data)\n",
    "print(tokenizer2.word_index)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have different lengths and lost of complexity. We really need a way to take any sentence and map it into a fixed vector length that is always the same size. We could pad our input sentences to be the same length then the output is always the same. Padding support is provided directly in keras/tf. Also, the data we have can be large and complex paragraphs vs individual words or sentences. We need to manipulate the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through all these issues would be alot of work during our workshop. Fortunately, there are modular building blockes for creating sentence embeddings already available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to create sentence embeddings that have semantic meaning. Simply converting sentences into numbers directly feels devoid of the richness in language. Also, we need to find a way to capture more \"context\" in a finite sized vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Word Embeddings\n",
    "\n",
    "Word embeddings capture semantic meaning by creating a numerical description of a word that takes into account all of the places the word is observed in various large documents--search as wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load some BERT embedding and show the embedding concept for a word using BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
