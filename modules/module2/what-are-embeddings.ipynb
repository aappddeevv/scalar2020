{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Embeddings Using Tensorflow\n",
    "\n",
    "We will create some simple embeddings to help illustrate how to change text to numbers so we can process it.\n",
    "\n",
    "You can watch https://www.youtube.com/watch?v=fNxaJsNG3-s to learn about this more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization - Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Please use a spoon\",\n",
    "    \"Please use a napkin!\",\n",
    "    \"Let's go to the store and buy some groceries\",\n",
    "    \"The park is closed at night.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index list all the words in the sentences and assigns them a number, but that's not enough for processing. You should also notice that these particular functions removed non-character values such as `!` from the text automatically. Not all functions do this so sometimes you need to prep your data more carefully.\n",
    "\n",
    "You may also notice that `Let's` was not changed and the apostrophe was included in the \"word\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'please': 1, 'use': 2, 'a': 3, 'the': 4, 'spoon': 5, 'napkin': 6, \"let's\": 7, 'go': 8, 'to': 9, 'store': 10, 'and': 11, 'buy': 12, 'some': 13, 'groceries': 14, 'park': 15, 'is': 16, 'closed': 17, 'at': 18, 'night': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences (Sequences of Words) and What's `<OOV>`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence is now a sequence of numbers based on the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5], [1, 2, 3, 6], [7, 8, 9, 4, 10, 11, 12, 13, 14], [4, 15, 16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know try to sequence a sentence with words that have not been seen before, the word is essentially dropped in the output vector. So you need to have a large index to capture all the possible world. But a large dictionary can never be large enough. Let's create a new tokenizer that has a placeholder for missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer2.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'please': 2, 'use': 3, 'a': 4, 'the': 5, 'spoon': 6, 'napkin': 7, \"let's\": 8, 'go': 9, 'to': 10, 'store': 11, 'and': 12, 'buy': 13, 'some': 14, 'groceries': 15, 'park': 16, 'is': 17, 'closed': 18, 'at': 19, 'night': 20}\n",
      "[[1, 5, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\"Soon, the moon shall rise\"]\n",
    "test_seq = tokenizer2.texts_to_sequences(test_data)\n",
    "print(tokenizer2.word_index)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have different lengths and lost of complexity. We really need a way to take any sentence and map it into a fixed vector length that is always the same size. We could pad our input sentences to be the same length then the output is always the same. Padding support is provided directly in keras/tf. Also, the data we have can be large and complex paragraphs vs individual words or sentences. We need to manipulate the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through all these issues would be alot of work during our workshop. Fortunately, there are modular building blockes for creating sentence embeddings already available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to create sentence embeddings that have semantic meaning. Simply converting sentences into numbers directly feels devoid of the richness in language. Also, we need to find a way to capture more \"context\" in a finite sized vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Word Embeddings\n",
    "\n",
    "Word embeddings capture semantic meaning by creating a numerical description of a word that takes into account all of the places the word is observed in various large documents--search as wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load some BERT embedding and show the embedding concept for a word using BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
