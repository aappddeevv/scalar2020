{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reg Data Prep\n",
    "\n",
    "We already download our data and created CSV files but we should take a quick look at them and see how easy they are to work with. At some point, we will need to create some refined datasets that augment the raw text with additional NLP derived content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in a container that only had tensorflow installed you would need to install additional software to run some of the functions below. In the last notebook we saw that you can tokenize sentences using `Tokenizer` from the tf python library. Our regulatory text has alot of paragrahs in it. We *may* need to chop up our larger paragraphs into smaller units to work with.\n",
    "\n",
    "NLP algorithms do *not* do well with long-form documents and there are many schemes to scale up \"word\" embeddings to sentence embeddings and more importantly \"whole document\" embeddings. While there are some obvious ways to combine word embeddings to get sentence embeddings, they do not work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run install commands, these run in the container. Assuming the container has its own python installation, these commands install some packages but they are not installed into the image. They will be present in the container if you pause/unpause a container though. In our case, we don't care too much as we are doing some exploratory work. If the packages are already present, no updates will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q spacy\n",
    "!pip install -q nltk\n",
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our files are in csv form, lets load them up and print out the first entries.\n",
    "\n",
    "First the US regulatory data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs into Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print('NTLK Version: %s' % nltk.__version__)\n",
    "for article in [article, article2, article3]:\n",
    "    print('Original Article: %s' % (article))\n",
    "    print()\n",
    "doc = sent_tokenize(article)\n",
    "    for i, token in enumerate(doc):\n",
    "        print('-->Sentence %d: %s' % (i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize does word tokenization but does not pick up sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "Output: ['God is Great!', 'I won a lottery ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in [article, article2, article3]:\n",
    "    print('Original Article: %s' % (article))\n",
    "    print()\n",
    "    doc = spacy_nlp(article)\n",
    "    for i, token in enumerate(doc.sents):\n",
    "        print('-->Sentence %d: %s' % (i, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another spacy?\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "text = '''Your text here'''\n",
    "tokens = nlp(text)\n",
    "\n",
    "for sent in tokens.sents:\n",
    "    print(sent.string.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
